{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Data Collection and Preprocessing for Foundation Model Pre‑Training\n",
    "\n",
    "This notebook demonstrates how to build a data preprocessing pipeline for pre‑training transformer‑based foundation models. It mirrors the original Python script, split into explanatory cells. You can run the parts you need and adapt them for larger corpora (streaming-friendly).\n",
    "\n",
    "**What you'll see:**\n",
    "- Collect raw text via Hugging Face `datasets` (streaming)\n",
    "- Clean & normalize text (lowercasing, strip HTML/URLs, collapse whitespace, etc.)\n",
    "- Deduplicate and filter too‑short documents\n",
    "- Tokenize with a Hugging Face `AutoTokenizer`; chunk to fixed block size\n",
    "- Build a PyTorch `Dataset` + `DataLoader` with padding & masks\n",
    "- Save a few tokenized batches to `.pt` for inspection\n",
    "\n",
    "> **Note**: This notebook expects `datasets`, `transformers`, and `torch` to be installed in your environment. Install them with the cell below if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: install dependencies (uncomment to run in a fresh environment)\n",
    "# !pip install -U datasets transformers torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Logger\n",
    "We import lazily and keep placeholders so the notebook can be read without immediately having all libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f334d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import argparse\n",
    "import collections\n",
    "import html\n",
    "import logging\n",
    "import re\n",
    "import sys\n",
    "from typing import Iterable, Iterator, List, Tuple\n",
    "\n",
    "import datasets  # type: ignore\n",
    "import transformers  # type: ignore\n",
    "import torch  # type: ignore\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except ImportError:  # pragma: no cover - fallback without tqdm\n",
    "    def tqdm(iterable=None, **kwargs):\n",
    "        return iterable if iterable is not None else []\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d2432d",
   "metadata": {},
   "source": [
    "## 1) Stream a dataset from the Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b165794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_stream(dataset: str, dataset_name: str | None = None, split: str = \"train\") -> Iterable[str]:\n",
    "    \"\"\"Load text documents from a Hugging Face dataset using streaming.\"\"\"\n",
    "    if datasets is None:\n",
    "        raise ImportError(\"The 'datasets' library is required to load datasets.\")\n",
    "    logger.info(\"Loading dataset %s%s with split=%s\", dataset, f\"[{dataset_name}]\" if dataset_name else \"\", split)\n",
    "    data = datasets.load_dataset(dataset, dataset_name, split=split, streaming=True)\n",
    "    for example in data:\n",
    "        text = example.get(\"text\")\n",
    "        if text is None:\n",
    "            for value in example.values():\n",
    "                if isinstance(value, str):\n",
    "                    text = value\n",
    "                    break\n",
    "        if text is None:\n",
    "            continue\n",
    "        yield text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87745b3c",
   "metadata": {},
   "source": [
    "## 2) Cleaning utilities\n",
    "Basic cleaning: unescape HTML, strip tags/URLs, lowercase, ASCII normalize, remove punctuation, collapse whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb71e040",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodedata_normalize(text: str) -> str:\n",
    "    \"\"\"Normalize accented characters to ASCII equivalents (for primarily English corpora).\"\"\"\n",
    "    import unicodedata\n",
    "    normalized = unicodedata.normalize(\"NFKD\", text)\n",
    "    return normalized.encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = html.unescape(text)\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)  # remove HTML tags\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", text)  # remove URLs\n",
    "    text = text.lower()\n",
    "    text = unicodedata_normalize(text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # remove punctuation/symbols\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29801ee7",
   "metadata": {},
   "source": [
    "## 3) Deduplication & filtering short docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fb613ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_documents(docs: Iterable[str]) -> List[str]:\n",
    "    seen: set[str] = set()\n",
    "    unique_docs: List[str] = []\n",
    "    for doc in tqdm(docs, desc=\"Deduplicating documents\"):\n",
    "        if doc not in seen:\n",
    "            seen.add(doc)\n",
    "            unique_docs.append(doc)\n",
    "    return unique_docs\n",
    "\n",
    "def filter_short_documents(docs: Iterable[str], min_words: int = 50) -> List[str]:\n",
    "    filtered: List[str] = []\n",
    "    for doc in tqdm(docs, desc=\"Deduplicating documents\"):\n",
    "        if len(doc.split()) >= min_words:\n",
    "            filtered.append(doc)\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d844d1",
   "metadata": {},
   "source": [
    "## 4) Tokenize and chunk with Hugging Face `AutoTokenizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f96dbb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_chunk(docs: Iterable[str], tokenizer_name: str, block_size: int) -> List[List[int]]:\n",
    "    if transformers is None:\n",
    "        raise ImportError(\"The 'transformers' library is required for tokenisation.\")\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    tokenised_blocks: List[List[int]] = []\n",
    "    for doc in tqdm(docs, desc=\"Tokenizing documents\"):\n",
    "        tokens = tokenizer.encode(doc, add_special_tokens=False)\n",
    "        for i in range(0, len(tokens), block_size):\n",
    "            chunk = tokens[i : i + block_size]\n",
    "            tokenised_blocks.append(chunk)\n",
    "    return tokenised_blocks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ab43d",
   "metadata": {},
   "source": [
    "## 5) PyTorch Dataset, collate function, and saving sample batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f04ae893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset:\n",
    "    \"\"\"PyTorch-like dataset for tokenised text blocks.\"\"\"\n",
    "    def __init__(self, token_blocks: List[List[int]], pad_token_id: int = 0) -> None:\n",
    "        if torch is None:\n",
    "            raise ImportError(\"The 'torch' library is required for creating the dataset.\")\n",
    "        self.token_blocks = token_blocks\n",
    "        self.pad_token_id = pad_token_id\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.token_blocks)\n",
    "    def __getitem__(self, idx: int):\n",
    "        ids = self.token_blocks[idx]\n",
    "        return torch.tensor(ids, dtype=torch.long), torch.ones(len(ids), dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    if torch is None:\n",
    "        raise ImportError(\"The 'torch' library is required for collation.\")\n",
    "    input_ids, attention_masks = zip(*batch)\n",
    "    max_len = max(seq.size(0) for seq in input_ids)\n",
    "    padded_ids, padded_masks = [], []\n",
    "    for ids, mask in zip(input_ids, attention_masks):\n",
    "        pad_length = max_len - ids.size(0)\n",
    "        padded_ids.append(torch.cat([ids, torch.full((pad_length,), 0, dtype=torch.long)]))\n",
    "        padded_masks.append(torch.cat([mask, torch.zeros(pad_length, dtype=torch.long)]))\n",
    "    return {\"input_ids\": torch.stack(padded_ids), \"attention_mask\": torch.stack(padded_masks)}\n",
    "\n",
    "def save_sample_batches(dataloader, num_batches: int | None, output_path: str) -> int:\n",
    "    if torch is None:\n",
    "        raise ImportError(\"The 'torch' library is required for saving sample batches.\")\n",
    "    saved_batches: List[dict] = []\n",
    "    saved_count = 0\n",
    "    for i, batch in enumerate(tqdm(dataloader, desc=\"Saving sample batches\", total=num_batches)):\n",
    "        if num_batches is not None and i >= num_batches:\n",
    "            break\n",
    "        saved_batches.append({k: v.clone().cpu() for k, v in batch.items()})\n",
    "        saved_count += 1\n",
    "    try:\n",
    "        torch.save(saved_batches, output_path)\n",
    "    except Exception:\n",
    "        import pickle\n",
    "        with open(output_path, \"wb\") as fh:\n",
    "            pickle.dump(saved_batches, fh)\n",
    "    return saved_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730066d",
   "metadata": {},
   "source": [
    "## 6) Full preprocessing pipeline (callable from this notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2e04651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(\n",
    "    dataset: str,\n",
    "    dataset_name: str | None,\n",
    "    tokenizer_name: str,\n",
    "    block_size: int,\n",
    "    min_words: int,\n",
    "    sample_batches: int,\n",
    "    output_path: str,\n",
    "    max_documents: int | None = None,\n",
    ") -> None:\n",
    "    docs_iter: Iterator[str] = load_dataset_stream(dataset, dataset_name)\n",
    "    cleaned_docs: List[str] = []\n",
    "    for i, doc in enumerate(tqdm(docs_iter, desc=\"Cleaning documents\", total=max_documents), start=1):\n",
    "        cleaned = clean_text(doc)\n",
    "        cleaned_docs.append(cleaned)\n",
    "        if max_documents is not None and i >= max_documents:\n",
    "            break\n",
    "    logger.info(\"Loaded %d documents\", len(cleaned_docs))\n",
    "    unique_docs = deduplicate_documents(cleaned_docs)\n",
    "    logger.info(\"After deduplication: %d documents\", len(unique_docs))\n",
    "    filtered_docs = filter_short_documents(unique_docs, min_words=min_words)\n",
    "    logger.info(\"After filtering short docs: %d documents\", len(filtered_docs))\n",
    "    token_blocks = tokenize_and_chunk(filtered_docs, tokenizer_name, block_size)\n",
    "    logger.info(\"Number of token blocks: %d\", len(token_blocks))\n",
    "    if transformers is None or torch is None:\n",
    "        raise ImportError(\"Both 'transformers' and 'torch' are required to continue.\")\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    pad_token_id = tokenizer.pad_token_id or 0\n",
    "    dataset_obj = TextDataset(token_blocks, pad_token_id=pad_token_id)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset_obj, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "    saved = save_sample_batches(dataloader, sample_batches, output_path)\n",
    "    logger.info(\"Saved %d batches to %s\", saved, output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Example configuration (edit and run)\n",
    "You can tweak these parameters for your environment/dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'wiki40b',\n",
       " 'dataset_name': 'en',\n",
       " 'tokenizer': 'gpt2',\n",
       " 'block_size': 1024,\n",
       " 'min_words': 50,\n",
       " 'sample_batches': 10,\n",
       " 'output': 'sample_dataset.pt',\n",
       " 'max_docs': 1000}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"dataset\": \"wiki40b\",          # e.g., 'openwebtext', 'c4', etc.\n",
    "    \"dataset_name\": \"en\",           # or None if single-config dataset\n",
    "    \"tokenizer\": \"gpt2\",            # or 'bert-base-uncased', etc.\n",
    "    \"block_size\": 1024,\n",
    "    \"min_words\": 50,\n",
    "    \"sample_batches\": 10,            # or None to get all\n",
    "    \"output\": \"sample_dataset.pt\",\n",
    "    \"max_docs\": 1000,                # limit for quick trial; set None for full stream\n",
    "}\n",
    "config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Run the full pipeline (may take time depending on dataset/network)\n",
    "Uncomment the cell below to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 02:42:48,174 - INFO - Loading dataset wiki40b[en] with split=train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a68a418340d48618a0b459f1fded4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-21 02:42:52,023 - INFO - Loaded 1000 documents\n",
      "2025-09-21 02:42:52,026 - INFO - After deduplication: 1000 documents\n",
      "2025-09-21 02:42:52,055 - INFO - After filtering short docs: 876 documents\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1420 > 1024). Running this sequence through the model will result in indexing errors\n",
      "2025-09-21 02:42:56,054 - INFO - Number of token blocks: 1184\n",
      "2025-09-21 02:42:56,588 - INFO - Saved 10 batches to sample_dataset.pt\n"
     ]
    }
   ],
   "source": [
    "_cfg = config\n",
    "preprocess_pipeline(\n",
    "    dataset=_cfg[\"dataset\"],\n",
    "    dataset_name=_cfg[\"dataset_name\"],\n",
    "    tokenizer_name=_cfg[\"tokenizer\"],\n",
    "    block_size=_cfg[\"block_size\"],\n",
    "    min_words=_cfg[\"min_words\"],\n",
    "    sample_batches=_cfg[\"sample_batches\"],\n",
    "    output_path=_cfg[\"output\"],\n",
    "    max_documents=_cfg[\"max_docs\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes & Tips\n",
    "- For multilingual corpora, consider **not** converting to ASCII in `unicodedata_normalize`.\n",
    "- If your tokenizer's `pad_token_id` is not 0, you can modify `collate_fn` to use it.\n",
    "- For large-scale jobs, shard tokenized blocks to disk instead of keeping all in memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
